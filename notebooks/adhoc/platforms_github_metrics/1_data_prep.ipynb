{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part 1: Data Fetch & Preparation\n",
    "--------------------------------\n",
    "This script:\n",
    " 1) Fetches data from the GitHub API (commits, issues, PRs, etc.) using GitHubFetcher.\n",
    " 2) Builds metrics and time-series data via GitHubMetricsBuilder.\n",
    " 3) Writes out multiple CSV files (commits, issues, pulls, timeseries, etc.) into a 'data/' folder.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "from typing import Optional, Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from op_analytics.coreutils.logger import structlog\n",
    "from op_analytics.coreutils.request import new_session, get_data\n",
    "from op_analytics.coreutils.threads import run_concurrently\n",
    "\n",
    "# -------------------------------------\n",
    "# Rate Limit Helpers\n",
    "# -------------------------------------\n",
    "def get_rate_limit_info(session: requests.Session, token: Optional[str] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Fetch the current GitHub rate limit info from /rate_limit endpoint.\n",
    "    Returns a dict like: { 'limit': ..., 'remaining': ..., 'reset': ... } for the 'core' resource.\n",
    "    \"\"\"\n",
    "    headers = {\"Accept\": \"application/vnd.github+json\"}\n",
    "    if token:\n",
    "        headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "\n",
    "    rate_limit_url = \"https://api.github.com/rate_limit\"\n",
    "    resp = session.get(rate_limit_url, headers=headers)\n",
    "    resp.raise_for_status()\n",
    "\n",
    "    data = resp.json()\n",
    "    core_data = data[\"resources\"][\"core\"]\n",
    "    reset_time = datetime.fromtimestamp(core_data[\"reset\"], tz=timezone.utc).strftime('%Y-%m-%d %H:%M:%S %Z')\n",
    "    return {\n",
    "        \"limit\": core_data[\"limit\"],\n",
    "        \"remaining\": core_data[\"remaining\"],\n",
    "        \"reset\": reset_time,  # Human-readable time\n",
    "    }\n",
    "\n",
    "def wait_for_rate_limit(session: requests.Session, token: Optional[str] = None) -> None:\n",
    "    \"\"\"\n",
    "    Checks your 'core' REST rate limit. If remaining=0, waits until reset time.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        rate_info = get_rate_limit_info(session, token=token)\n",
    "        limit = rate_info[\"limit\"]\n",
    "        remaining = rate_info[\"remaining\"]\n",
    "        reset_time_str = rate_info[\"reset\"]\n",
    "\n",
    "        now_utc = datetime.now(timezone.utc)\n",
    "        print(f\"[RateLimit] Current UTC time: {now_utc.strftime('%Y-%m-%d %H:%M:%S %Z')}\")\n",
    "        print(f\"[RateLimit] Core usage: {remaining}/{limit} calls remaining.\")\n",
    "        print(f\"[RateLimit] Reset time: {reset_time_str}\")\n",
    "\n",
    "        if remaining == 0:\n",
    "            reset_dt = datetime.strptime(reset_time_str, '%Y-%m-%d %H:%M:%S %Z').replace(tzinfo=timezone.utc)\n",
    "            wait_seconds = (reset_dt - now_utc).total_seconds()\n",
    "            if wait_seconds > 0:\n",
    "                print(f\"[RateLimit] Exhausted. Waiting ~{wait_seconds:.0f}s until reset at {reset_dt}.\")\n",
    "                print(f\"[RateLimit] ETA on reset: {reset_time_str}\")\n",
    "                time.sleep(wait_seconds + 2)  # small buffer\n",
    "            else:\n",
    "                print(\"[RateLimit] Wait time is zero or negative, presumably just reset.\")\n",
    "        else:\n",
    "            break\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# GitHubFetcher\n",
    "# -------------------------------------\n",
    "class GitHubFetcher:\n",
    "    \"\"\"Fetch data from the GitHub API using concurrency and custom request logic.\"\"\"\n",
    "\n",
    "    def __init__(self, token: str, owner: str, repos: List[str]):\n",
    "        self.token = token\n",
    "        self.owner = owner\n",
    "        self.repos = repos\n",
    "        # We'll reuse a single session across all fetches\n",
    "        self.session = new_session()\n",
    "        self.session.headers.update({\"Authorization\": f\"token {self.token}\"})\n",
    "   \n",
    "    def _fetch_all_pages(self, initial_url: str) -> List:\n",
    "        \"\"\"\n",
    "        Fetch all pages for a given endpoint concurrently:\n",
    "        1) Fetch the initial page (serially).\n",
    "        2) Parse the Link header for rel=\"last\" to determine total pages (if available).\n",
    "        3) Generate URLs for all pages (2..N) or fetch a single 'next' page if there's no 'last' link.\n",
    "        4) Fetch them concurrently and combine results.\n",
    "        \"\"\"\n",
    "        # Single-page (initial) fetch\n",
    "        first_page_data = self._fetch_single_page_with_retry(initial_url)\n",
    "        all_items = first_page_data if isinstance(first_page_data, list) else [first_page_data]\n",
    "\n",
    "        link_header = self.session.head(initial_url).headers.get(\"Link\", None)\n",
    "        if not link_header:\n",
    "            # Possibly the HEAD approach was not needed or didn't find pagination, fallback\n",
    "            return all_items\n",
    "\n",
    "        # Attempt to find 'rel=\"last\"'\n",
    "        last_link = None\n",
    "        for part in link_header.split(\",\"):\n",
    "            if 'rel=\"last\"' in part:\n",
    "                start = part.find('<') + 1\n",
    "                end = part.find('>')\n",
    "                last_link = part[start:end]\n",
    "                break\n",
    "\n",
    "        if not last_link:\n",
    "            # If no 'last', maybe there's just 'next'\n",
    "            next_link = None\n",
    "            for part in link_header.split(\",\"):\n",
    "                if 'rel=\"next\"' in part:\n",
    "                    start = part.find('<') + 1\n",
    "                    end = part.find('>')\n",
    "                    next_link = part[start:end]\n",
    "                    break\n",
    "            if not next_link:\n",
    "                return all_items  # no next link => only one page\n",
    "\n",
    "            # Single next page scenario\n",
    "            return all_items + self._fetch_pages_in_parallel([next_link])\n",
    "\n",
    "        # There's a 'last' link. Let's parse out total pages from it.\n",
    "        from urllib.parse import urlparse, parse_qs\n",
    "        parsed_last = urlparse(last_link)\n",
    "        qs_last = parse_qs(parsed_last.query)\n",
    "        last_page = int(qs_last.get('page', [1])[0])\n",
    "\n",
    "        if last_page <= 1:\n",
    "            return all_items  # only one page total\n",
    "\n",
    "        # Build pages for 2..N\n",
    "        parsed_init = urlparse(initial_url)\n",
    "        qs_init = parse_qs(parsed_init.query)\n",
    "        qs_init.pop('page', None)\n",
    "\n",
    "        def build_page_url(page_num: int) -> str:\n",
    "            new_qs = qs_init.copy()\n",
    "            new_qs['page'] = str(page_num)\n",
    "            from urllib.parse import urlencode\n",
    "            query_str = urlencode(new_qs, doseq=True)\n",
    "            return f\"{parsed_init.scheme}://{parsed_init.netloc}{parsed_init.path}?{query_str}\"\n",
    "\n",
    "        all_page_urls = [build_page_url(p) for p in range(2, last_page + 1)]\n",
    "        results = self._fetch_pages_in_parallel(all_page_urls)\n",
    "        all_items.extend(results)\n",
    "        return all_items\n",
    "\n",
    "    def _fetch_pages_in_parallel(self, urls: List[str]) -> List:\n",
    "        \"\"\"Helper to fetch multiple page URLs concurrently with built-in wait-and-retry logic.\"\"\"\n",
    "        if not urls:\n",
    "            return []\n",
    "\n",
    "        def fetch_func(url: str) -> List:\n",
    "            data = self._fetch_single_page_with_retry(url)\n",
    "            return data if isinstance(data, list) else [data]\n",
    "\n",
    "        # Prepare concurrency targets\n",
    "        targets = {f\"page_{i+1}\": url for i, url in enumerate(urls)}\n",
    "        concurrency_results = run_concurrently(fetch_func, targets=targets, max_workers=4)\n",
    "\n",
    "        merged_items = []\n",
    "        for _, items in concurrency_results.items():\n",
    "            merged_items.extend(items)\n",
    "        return merged_items\n",
    "\n",
    "    def _fetch_single_page_with_retry(self, page_url: str) -> List | Dict:\n",
    "        \"\"\"\n",
    "        Manually handle retries by looping until we succeed or \n",
    "        decide to give up. Each iteration calls wait_for_rate_limit.\n",
    "        \"\"\"\n",
    "        max_attempts = 5\n",
    "        attempt = 0\n",
    "        backoff_seconds = 5\n",
    "\n",
    "        while True:\n",
    "            attempt += 1\n",
    "            try:\n",
    "                wait_for_rate_limit(self.session, token=self.token)\n",
    "                return get_data(session=self.session, url=page_url, retry_attempts=1)\n",
    "            except Exception as e:\n",
    "                print(f\"Error in fetching {page_url}. Attempt {attempt}/{max_attempts}: {e}\")\n",
    "                if attempt >= max_attempts:\n",
    "                    print(\"[_fetch_single_page_with_retry] Exceeded max attempts. Raising.\")\n",
    "                    raise\n",
    "                else:\n",
    "                    print(f\"Sleeping {backoff_seconds}s before retrying.\")\n",
    "                    time.sleep(backoff_seconds)\n",
    "                    backoff_seconds *= 2  # exponential backoff\n",
    "\n",
    "    def fetch_commits(self, repo: str, since: Optional[str] = None) -> List[Dict]:\n",
    "        url = f'https://api.github.com/repos/{self.owner}/{repo}/commits?state=all'\n",
    "        if since:\n",
    "            url += f'&since={since}'\n",
    "        return self._fetch_all_pages(url)\n",
    "\n",
    "    def fetch_issues(self, repo: str, since: Optional[str] = None) -> List[Dict]:\n",
    "        url = f'https://api.github.com/repos/{self.owner}/{repo}/issues?state=all'\n",
    "        if since:\n",
    "            url += f'&since={since}'\n",
    "        return self._fetch_all_pages(url)\n",
    "\n",
    "    def fetch_pulls(self, repo: str, since: Optional[str] = None) -> List[Dict]:\n",
    "        url = f'https://api.github.com/repos/{self.owner}/{repo}/pulls?state=all&sort=updated&direction=desc'\n",
    "        if since:\n",
    "            url += f'&since={since}'\n",
    "        return self._fetch_all_pages(url)\n",
    "\n",
    "    def fetch_releases(self, repo: str) -> List[Dict]:\n",
    "        url = f'https://api.github.com/repos/{self.owner}/{repo}/releases?state=all'\n",
    "        return self._fetch_all_pages(url)\n",
    "\n",
    "    def fetch_pr_comments(self, repo: str, pr_num: int) -> List[Dict]:\n",
    "        url = f'https://api.github.com/repos/{self.owner}/{repo}/pulls/{pr_num}/comments'\n",
    "        return self._fetch_all_pages(url)\n",
    "\n",
    "    def fetch_pr_reviews(self, repo: str, pr_num: int) -> List[Dict]:\n",
    "        url = f'https://api.github.com/repos/{self.owner}/{repo}/pulls/{pr_num}/reviews'\n",
    "        return self._fetch_all_pages(url)\n",
    "\n",
    "\n",
    "# -------------------------------------\n",
    "# GitHubMetricsBuilder\n",
    "# -------------------------------------\n",
    "log = structlog.get_logger()\n",
    "\n",
    "class GitHubMetricsBuilder:\n",
    "    \"\"\"Build DataFrames and compute metrics for GitHub repos, using concurrency and 'since' logic.\"\"\"\n",
    "\n",
    "    def __init__(self, token: str, owner: str, repos: List[str]):\n",
    "        self.fetcher = GitHubFetcher(token, owner, repos)\n",
    "        # DataFrame placeholders\n",
    "        self.commits = pd.DataFrame()\n",
    "        self.issues = pd.DataFrame()\n",
    "        self.pulls = pd.DataFrame()\n",
    "        self.releases = pd.DataFrame()\n",
    "        self.pr_comments = pd.DataFrame()\n",
    "\n",
    "    def _fetch_data_for_repo(self, repo: str, since: Optional[str]) -> Dict[str, pd.DataFrame]:\n",
    "        commits_data = self.fetcher.fetch_commits(repo, since=since)\n",
    "        issues_data = self.fetcher.fetch_issues(repo, since=since)\n",
    "        pulls_data = self.fetcher.fetch_pulls(repo, since=since)\n",
    "        releases_data = self.fetcher.fetch_releases(repo)\n",
    "\n",
    "        def df_with_repo(data_list, repo_str):\n",
    "            df = pd.DataFrame(data_list) if data_list else pd.DataFrame()\n",
    "            if not df.empty:\n",
    "                df['repo'] = repo_str\n",
    "            return df\n",
    "\n",
    "        return {\n",
    "            'commits' : df_with_repo(commits_data, repo),\n",
    "            'issues'  : df_with_repo(issues_data,  repo),\n",
    "            'pulls'   : df_with_repo(pulls_data,   repo),\n",
    "            'releases': df_with_repo(releases_data,repo)\n",
    "        }\n",
    "\n",
    "    def fetch_all_data(self, since: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        # Create concurrency targets\n",
    "        targets = {repo: repo for repo in self.fetcher.repos}\n",
    "\n",
    "        def fetch_repo_func(r: str) -> Dict[str, pd.DataFrame]:\n",
    "            return self._fetch_data_for_repo(r, since)\n",
    "\n",
    "        concurrency_results = run_concurrently(fetch_repo_func, targets=targets, max_workers=32)\n",
    "        commits_df, issues_df, pulls_df, releases_df = pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "        for repo_name, repo_dict in concurrency_results.items():\n",
    "            ctemp = repo_dict['commits']\n",
    "            itemp = repo_dict['issues']\n",
    "            ptemp = repo_dict['pulls']\n",
    "            rtemp = repo_dict['releases']\n",
    "\n",
    "            if not ctemp.empty:\n",
    "                commits_df = pd.concat([commits_df, ctemp], ignore_index=True)\n",
    "            if not itemp.empty:\n",
    "                issues_df = pd.concat([issues_df, itemp], ignore_index=True)\n",
    "            if not ptemp.empty:\n",
    "                pulls_df = pd.concat([pulls_df, ptemp], ignore_index=True)\n",
    "            if not rtemp.empty:\n",
    "                releases_df = pd.concat([releases_df, rtemp], ignore_index=True)\n",
    "\n",
    "        return commits_df, issues_df, pulls_df, releases_df\n",
    "\n",
    "    def _extract_approval_info(self, reviews: List[Dict]) -> Tuple[pd.Timestamp, str]:\n",
    "        \"\"\"Find the first 'APPROVED' review's time and user.\"\"\"\n",
    "        approval = next((r for r in reviews if r.get('state', '').upper() == 'APPROVED'), None)\n",
    "        if approval:\n",
    "            return pd.to_datetime(approval['submitted_at'], errors='coerce'), approval['user']['login']\n",
    "        return pd.NaT, None\n",
    "\n",
    "    def _comments_to_df(self, comments: List[Dict]) -> pd.DataFrame:\n",
    "        \"\"\"Transform raw comment JSON into a DataFrame, marking is_bot.\"\"\"\n",
    "        if not comments:\n",
    "            return pd.DataFrame()\n",
    "        df = pd.DataFrame(comments)\n",
    "        df['user'] = df['user'].apply(lambda x: x.get('login'))\n",
    "        df['created_at'] = pd.to_datetime(df['created_at'], errors='coerce')\n",
    "        df['is_bot'] = df['user'].str.contains(\"[bot]\", case=False, na=False)\n",
    "        return df\n",
    "\n",
    "    def process_pull_requests(self, pulls: pd.DataFrame, since: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Process PR concurrency => fetch reviews/comments => metrics.\"\"\"\n",
    "        if pulls.empty:\n",
    "            return pulls, pd.DataFrame()\n",
    "\n",
    "        pulls['created_at'] = pd.to_datetime(pulls['created_at'], errors='coerce')\n",
    "        pulls['merged_at'] = pd.to_datetime(pulls['merged_at'], errors='coerce')\n",
    "        pulls['approval_time'] = pd.NaT\n",
    "        pulls['approver'] = None\n",
    "\n",
    "        since_dt = pd.to_datetime(since, utc=True, errors='coerce') if since else None\n",
    "        if since_dt is not None:\n",
    "            pulls = pulls[pulls['created_at'] >= since_dt].copy()\n",
    "        if pulls.empty:\n",
    "            return pulls, pd.DataFrame()\n",
    "\n",
    "        def build_target(row_idx, row):\n",
    "            return {\n",
    "                'row_idx': row_idx,\n",
    "                'repo': row['repo'],\n",
    "                'pr_number': row['number'],\n",
    "                'pr_url': row['url']\n",
    "            }\n",
    "\n",
    "        targets = {f\"pr_{i}\": build_target(i, pr_row) for i, pr_row in pulls.iterrows()}\n",
    "\n",
    "        def fetch_pr_data(pr_info: dict) -> dict:\n",
    "            \"\"\"\n",
    "            Each PR fetches reviews & comments (also with concurrency-level wait & retry),\n",
    "            then returns the relevant approval times & comments DF.\n",
    "            \"\"\"\n",
    "            row_idx = pr_info['row_idx']\n",
    "            repo = pr_info['repo']\n",
    "            pr_number = pr_info['pr_number']\n",
    "            pr_url = pr_info['pr_url']\n",
    "\n",
    "            output = {\n",
    "                'row_idx': row_idx,\n",
    "                'approval_time': pd.NaT,\n",
    "                'approver': None,\n",
    "                'comments_df': pd.DataFrame()\n",
    "            }\n",
    "            try:\n",
    "                reviews = self.fetcher.fetch_pr_reviews(repo, pr_number)\n",
    "                approval_time, approver = self._extract_approval_info(reviews)\n",
    "                output['approval_time'] = approval_time\n",
    "                output['approver'] = approver\n",
    "\n",
    "                raw_comments = self.fetcher.fetch_pr_comments(repo, pr_number)\n",
    "                cdf = self._comments_to_df(raw_comments)\n",
    "                if not cdf.empty:\n",
    "                    cdf['pull_request_number'] = pr_number\n",
    "                    cdf['repo'] = repo\n",
    "                    cdf['pull_request_url'] = pr_url\n",
    "                    output['comments_df'] = cdf\n",
    "\n",
    "            except Exception as ex:\n",
    "                log.error(f\"Failed to process PR #{pr_number} in {repo}\", exc_info=ex)\n",
    "\n",
    "            return output\n",
    "\n",
    "        concurrency_results = run_concurrently(fetch_pr_data, targets, max_workers=32)\n",
    "\n",
    "        all_comments_list = []\n",
    "        for pr_key, pr_dict in concurrency_results.items():\n",
    "            idx = pr_dict['row_idx']\n",
    "            pulls.at[idx, 'approval_time'] = pr_dict['approval_time']\n",
    "            pulls.at[idx, 'approver'] = pr_dict['approver']\n",
    "            cdf = pr_dict['comments_df']\n",
    "            if not cdf.empty:\n",
    "                all_comments_list.append(cdf)\n",
    "\n",
    "        comments_df = pd.concat(all_comments_list, ignore_index=True) if all_comments_list else pd.DataFrame()\n",
    "\n",
    "        pulls['time_to_merge_days'] = (pulls['merged_at'] - pulls['created_at']).dt.total_seconds() / (3600 * 24)\n",
    "        pulls['approval_time'] = pd.to_datetime(pulls['approval_time'], errors='coerce')\n",
    "        pulls['time_to_approval_days'] = (pulls['approval_time'] - pulls['created_at']).dt.total_seconds() / (3600 * 24)\n",
    "\n",
    "        if not comments_df.empty:\n",
    "            non_bot_comments = comments_df[~comments_df['is_bot']].copy()\n",
    "            non_bot_comments.sort_values(by='created_at', inplace=True)\n",
    "            first_non_bot_comments = non_bot_comments.groupby('pull_request_url', as_index=False).first()\n",
    "\n",
    "            pulls = pulls.merge(\n",
    "                first_non_bot_comments[['pull_request_url', 'user', 'created_at']],\n",
    "                left_on='url',\n",
    "                right_on='pull_request_url',\n",
    "                how='left',\n",
    "                suffixes=('', '_first_non_bot')\n",
    "            )\n",
    "\n",
    "            pulls['time_to_first_non_bot_comment_days'] = (\n",
    "                pulls['created_at_first_non_bot'] - pulls['created_at']\n",
    "            ).dt.total_seconds() / (3600 * 24)\n",
    "            pulls.rename(columns={'user': 'first_non_bot_comment_user'}, inplace=True)\n",
    "        else:\n",
    "            pulls['time_to_first_non_bot_comment_days'] = np.nan\n",
    "            pulls['first_non_bot_comment_user'] = None\n",
    "\n",
    "        return pulls, comments_df\n",
    "\n",
    "    def process_releases(self, releases: pd.DataFrame) -> pd.DataFrame:\n",
    "        if releases.empty:\n",
    "            return releases\n",
    "        releases['created_at'] = pd.to_datetime(releases['created_at'], errors='coerce')\n",
    "        releases['major_version'] = (\n",
    "            releases['tag_name']\n",
    "            .str.extract(r'(\\d+\\.\\d+\\.\\d+)')[0]\n",
    "            .str.split('.')\n",
    "            .str[0]\n",
    "            .astype(float, errors='ignore')\n",
    "        ).fillna(0)\n",
    "        releases.sort_values(by=['repo', 'created_at'], inplace=True)\n",
    "        releases['version_change'] = releases.groupby('repo')['major_version'].diff().fillna(0) != 0\n",
    "        releases['major_version_change_time'] = releases.groupby('repo')['created_at'].diff()\n",
    "        return releases\n",
    "\n",
    "    def build_developer_experience_metrics(self, pulls: pd.DataFrame, comments_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        if comments_df.empty:\n",
    "            pulls['comments_count'] = 0\n",
    "        else:\n",
    "            non_bot_comments = comments_df[~comments_df['is_bot']]\n",
    "            comment_counts = (non_bot_comments.groupby('pull_request_url').size().reset_index(name='comments_count'))\n",
    "            pulls = pulls.merge(comment_counts, left_on='url', right_on='pull_request_url', how='left')\n",
    "            pulls['comments_count'] = pulls['comments_count'].fillna(0)\n",
    "        pulls['approved'] = pulls['time_to_approval_days'].notna()\n",
    "        return pulls\n",
    "\n",
    "    def build_time_series_metrics(self, pulls: pd.DataFrame) -> pd.DataFrame:\n",
    "        if pulls.empty:\n",
    "            return pd.DataFrame()\n",
    "\n",
    "        pulls['date'] = pulls['created_at'].dt.date\n",
    "        grouped = pulls.groupby(['repo', 'date'])\n",
    "        timeseries = grouped.agg(\n",
    "            number_of_prs=('number', 'count'),\n",
    "            avg_time_to_merge_days=('time_to_merge_days', 'mean'),\n",
    "            median_time_to_merge_days=('time_to_merge_days', 'median'),\n",
    "            min_time_to_merge_days=('time_to_merge_days', 'min'),\n",
    "            max_time_to_merge_days=('time_to_merge_days', 'max'),\n",
    "            avg_time_to_first_non_bot_comment_days=('time_to_first_non_bot_comment_days', 'mean'),\n",
    "            median_time_to_first_non_bot_comment_days=('time_to_first_non_bot_comment_days', 'median'),\n",
    "            avg_time_to_approval_days=('time_to_approval_days', 'mean'),\n",
    "            median_time_to_approval_days=('time_to_approval_days', 'median'),\n",
    "            avg_comments_per_pr=('comments_count', 'mean'),\n",
    "            approval_ratio=('approved', 'mean')\n",
    "        ).reset_index()\n",
    "        return timeseries\n",
    "\n",
    "    def get_metrics(self, since: Optional[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "        self.commits, self.issues, self.pulls, self.releases = self.fetch_all_data(since=since)\n",
    "        self.pulls, self.pr_comments = self.process_pull_requests(self.pulls, since=since)\n",
    "        self.releases = self.process_releases(self.releases)\n",
    "        self.pulls = self.build_developer_experience_metrics(self.pulls, self.pr_comments)\n",
    "        timeseries_metrics = self.build_time_series_metrics(self.pulls)\n",
    "        return {\n",
    "            'commits': self.commits,\n",
    "            'issues': self.issues,\n",
    "            'pull_requests': self.pulls,\n",
    "            'pr_comments': self.pr_comments,\n",
    "            'releases': self.releases,\n",
    "            'timeseries_metrics': timeseries_metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform data fetch & store CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from op_analytics.coreutils.env.vault import init\n",
    "import os\n",
    "init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provide your GH personal access token\n",
    "TOKEN = os.getenv(\"GITHUB_TOKEN\")\n",
    "owner = \"ethereum-optimism\"\n",
    "\n",
    "# Repos you want to process\n",
    "all_repos = [\n",
    "    \"optimism\",\n",
    "    \"supersim\",\n",
    "    \"superchainerc20-starter\",\n",
    "    \"superchain-registry\",\n",
    "    \"superchain-ops\",\n",
    "    \"docs\",\n",
    "    \"specs\",\n",
    "    \"design-docs\",\n",
    "    \"infra\",\n",
    "]\n",
    "\n",
    "# Create a session for rate-limit checks\n",
    "session = new_session()\n",
    "get_rate_limit_info(session, token=TOKEN)\n",
    "wait_for_rate_limit(session, token=TOKEN)\n",
    "\n",
    "# If you want to limit data from a certain date:\n",
    "since_date = \"2024-01-01T00:00:00Z\"\n",
    "\n",
    "for repo in all_repos:\n",
    "    print(f\"Processing {repo} ...\")\n",
    "    repos = [repo]\n",
    "    rate_limit = get_rate_limit_info(session, token=TOKEN)\n",
    "    while rate_limit['remaining'] < 2000:\n",
    "        print(f\"Rate limit remaining: {rate_limit['remaining']}. Waiting for reset...\")\n",
    "        time.sleep(30)\n",
    "        rate_limit = get_rate_limit_info(session, token=TOKEN)\n",
    "        print(f\"Rate limit remaining: {rate_limit['remaining']}\")\n",
    "    # Build metrics\n",
    "    builder = GitHubMetricsBuilder(TOKEN, owner, repos)\n",
    "    metrics = builder.get_metrics(since=since_date)\n",
    "\n",
    "    commits_df = metrics['commits']\n",
    "    issues_df = metrics['issues']\n",
    "    pulls_df = metrics['pull_requests']\n",
    "    pr_comments_df = metrics['pr_comments']\n",
    "    releases_df = metrics['releases']\n",
    "    timeseries_df = metrics['timeseries_metrics']\n",
    "\n",
    "    print(\"Commits shape:\", commits_df.shape)\n",
    "    print(\"Issues shape:\", issues_df.shape)\n",
    "    print(\"Pull Requests shape:\", pulls_df.shape)\n",
    "    print(\"Timeseries shape:\", timeseries_df.shape)\n",
    "    # Create data directory if it doesn't exist\n",
    "    os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "    # Save all DataFrames under data/ folder\n",
    "    commits_df.to_csv(f\"data/{repo}_commits.csv\", index=False)\n",
    "    issues_df.to_csv(f\"data/{repo}_issues.csv\", index=False)\n",
    "    pulls_df.to_csv(f\"data/{repo}_pulls.csv\", index=False)\n",
    "    pr_comments_df.to_csv(f\"data/{repo}_pr_comments.csv\", index=False)\n",
    "    releases_df.to_csv(f\"data/{repo}_releases.csv\", index=False)\n",
    "    timeseries_df.to_csv(f\"data/{repo}_timeseries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
